{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellUniqueIdByVincent": "ab8b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/omar/Downloads/SDR/Notebooks\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print current working directory to understand file paths\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellUniqueIdByVincent": "cd875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal data shape: (695552,)\n",
      "Constant Jammer data shape: (694278,)\n",
      "Pulsed Jammer data shape: (694642,)\n",
      "Test data shape: (200, 1)\n",
      "Test labels shape: (200,)\n",
      "Class distribution: [66 66 68]\n"
     ]
    }
   ],
   "source": [
    "# Define paths to the dataset files\n",
    "normal_path = \"../Dataset/training/Rssi_Normal.txt\"\n",
    "cj_path = \"../Dataset/training/Rssi_CJ.txt\"\n",
    "pj_path = \"../Dataset/training/Rssi_PJ.txt\"\n",
    "\n",
    "# Function to load the data\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        data = np.loadtxt(file_path)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# Load the datasets\n",
    "normal_data = load_data(normal_path)\n",
    "cj_data = load_data(cj_path)\n",
    "pj_data = load_data(pj_path)\n",
    "\n",
    "# Print the shapes to verify data loading\n",
    "print(f\"Normal data shape: {normal_data.shape}\")\n",
    "print(f\"Constant Jammer data shape: {cj_data.shape}\")\n",
    "print(f\"Pulsed Jammer data shape: {pj_data.shape}\")\n",
    "\n",
    "# Select random samples from each class (balanced)\n",
    "samples_per_class = 200 // 3  # Approximately equal samples per class\n",
    "remaining_samples = 200 - (samples_per_class * 3)  # Handle any remainder\n",
    "\n",
    "# Select random indices for each class\n",
    "normal_indices = np.random.choice(len(normal_data), samples_per_class, replace=False)\n",
    "cj_indices = np.random.choice(len(cj_data), samples_per_class, replace=False)\n",
    "pj_indices = np.random.choice(len(pj_data), samples_per_class + remaining_samples, replace=False)\n",
    "\n",
    "# Extract the samples\n",
    "normal_samples = normal_data[normal_indices]\n",
    "cj_samples = cj_data[cj_indices]\n",
    "pj_samples = pj_data[pj_indices]\n",
    "\n",
    "# Create labels (0: Normal, 1: Constant Jammer, 2: Pulsed Jammer)\n",
    "normal_labels = np.zeros(samples_per_class)\n",
    "cj_labels = np.ones(samples_per_class)\n",
    "pj_labels = np.full(samples_per_class + remaining_samples, 2)\n",
    "\n",
    "# Combine samples and labels\n",
    "X_test = np.concatenate([normal_samples, cj_samples, pj_samples])\n",
    "y_test = np.concatenate([normal_labels, cj_labels, pj_labels])\n",
    "\n",
    "# Reshape X_test to match model input requirements (assuming 1D input)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1)\n",
    "\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_test.astype(int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "c797d"
   },
   "outputs": [],
   "source": [
    "# Check the model's input shape requirements\n",
    "def get_model_input_shape(model):\n",
    "    \"\"\"Extract the expected input shape from the model\"\"\"\n",
    "    # Get the first layer\n",
    "    first_layer = model.layers[0]\n",
    "    # Get the input shape\n",
    "    input_shape = first_layer.input_shape\n",
    "    return input_shape\n",
    "\n",
    "# This function will be used after loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "e919e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: Unrecognized keyword arguments: ['batch_shape']\n",
      "\n",
      "Trying alternative model...\n",
      "Error loading alternative model: Unrecognized keyword arguments: ['batch_shape']\n",
      "\n",
      "Please check the model files and format.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model from the model directory\n",
    "model_path = \"../model/model_NewDataset.h5\"\n",
    "\n",
    "# Try to load the model with custom_objects parameter\n",
    "model = load_model(model_path, compile=False)\n",
    "print(f\"Successfully loaded model from {model_path}\")\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "db268"
   },
   "outputs": [],
   "source": [
    "# Check the model's expected input shape\n",
    "input_shape = get_model_input_shape(model)\n",
    "print(f\"Model expects input shape: {input_shape}\")\n",
    "\n",
    "# Examine the model's first few layers to understand structure\n",
    "for i, layer in enumerate(model.layers[:3]):\n",
    "    print(f\"Layer {i}: {layer.name}, Input shape: {layer.input_shape}, Output shape: {layer.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "317ed"
   },
   "outputs": [],
   "source": [
    "# Reshape X_test to match model input requirements\n",
    "# Check the dimensionality requirements\n",
    "if input_shape[1] is not None:  # If the model expects a specific sequence length\n",
    "    # Get sample dimensionality\n",
    "    sample_dim = X_test.shape[1] if len(X_test.shape) > 1 else 1\n",
    "    \n",
    "    # If samples are 1D and model expects 2D or 3D input\n",
    "    if len(input_shape) >= 3:\n",
    "        # Reshape to (samples, sequence_length, features)\n",
    "        # For Conv1D, we need at least 3 time steps for a kernel size of 3\n",
    "        sequence_length = max(3, input_shape[1]) if input_shape[1] is not None else 3\n",
    "        \n",
    "        # If each sample is just a single value, we need to expand it\n",
    "        if sample_dim == 1:\n",
    "            print(\"Expanding single values to sequences...\")\n",
    "            # Create sequences by repeating each value\n",
    "            X_test_expanded = np.repeat(X_test, sequence_length).reshape(-1, sequence_length)\n",
    "            # Add feature dimension if needed\n",
    "            if len(input_shape) == 3:  # Model expects (batch, seq_len, features)\n",
    "                X_test = X_test_expanded.reshape(-1, sequence_length, 1)\n",
    "            else:\n",
    "                X_test = X_test_expanded\n",
    "        else:\n",
    "            # If samples already have multiple values, reshape appropriately\n",
    "            if len(input_shape) == 3:  # Model expects (batch, seq_len, features)\n",
    "                X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "    \n",
    "    # If the model just expects a flattened input\n",
    "    elif len(input_shape) == 2:  # Model expects (batch, features)\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Reshaped test data to: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "d5ad5"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1) if y_pred_probs.shape[1] > 1 else np.round(y_pred_probs).astype(int).flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Display detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "target_names = ['Normal', 'Constant Jammer', 'Pulsed Jammer']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "0cff3"
   },
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Use a color map that makes it clear which values are higher\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(target_names))\n",
    "plt.xticks(tick_marks, target_names, rotation=45)\n",
    "plt.yticks(tick_marks, target_names)\n",
    "\n",
    "# Add text annotations to each cell\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, f\"{cm[i, j]} ({cm_normalized[i, j]:.2f})\",\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm_normalized[i, j] > 0.5 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "ed8ff"
   },
   "outputs": [],
   "source": [
    "# Create a bar chart comparing true vs predicted class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "true_counts = np.bincount(y_test.astype(int), minlength=3)\n",
    "pred_counts = np.bincount(y_pred.astype(int), minlength=3)\n",
    "\n",
    "x = np.arange(len(target_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, true_counts, width, label='True Labels')\n",
    "plt.bar(x + width/2, pred_counts, width, label='Predicted Labels')\n",
    "\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('True vs Predicted Class Distribution')\n",
    "plt.xticks(x, target_names)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noeticenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vincent": {
   "sessionId": "a05bb047237f018238e79d95_2025-05-28T12-21-03-648Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
